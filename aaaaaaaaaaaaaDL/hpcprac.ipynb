{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e846048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 10\n",
      "Sum: 55\n",
      "Average: 5.5\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "def parallel_reduction(data, operation):\n",
    "    num_threads = min(len(data), os.cpu_count())\n",
    "    chunk_size = (len(data) + num_threads - 1) // num_threads\n",
    "\n",
    "    # Divide data into chunks\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "    # Perform reduction on each chunk in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = list(executor.map(operation, chunks))\n",
    "\n",
    "    # Final reduction on aggregated results from all threads\n",
    "    return operation(results)\n",
    "\n",
    "def parallel_min(chunk):\n",
    "    return min(chunk)\n",
    "\n",
    "def parallel_max(chunk):\n",
    "    return max(chunk)\n",
    "\n",
    "def parallel_sum(chunk):\n",
    "    return sum(chunk)\n",
    "\n",
    "def parallel_average(chunk):\n",
    "    return sum(chunk) / len(chunk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1, 5, 3, 7, 9, 2, 4, 6, 8, 10]\n",
    "    \n",
    "    # Min operation\n",
    "    min_value = parallel_reduction(data, parallel_min)\n",
    "    print(\"Min:\", min_value)\n",
    "\n",
    "    # Max operation\n",
    "    max_value = parallel_reduction(data, parallel_max)\n",
    "    print(\"Max:\", max_value)\n",
    "\n",
    "    # Sum operation\n",
    "    sum_value = parallel_reduction(data, parallel_sum)\n",
    "    print(\"Sum:\", sum_value)\n",
    "\n",
    "    # Average operation\n",
    "    avg_value = parallel_reduction(data, parallel_average)\n",
    "    print(\"Average:\", avg_value)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the start vertex: 5\n",
      "Parallel DFS from vertex 5 : [5, 1, 2, 0, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Array\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, num_nodes):\n",
    "        self.adj_list = [[] for _ in range(num_nodes)]\n",
    "\n",
    "    def add_edge(self, src, dest):\n",
    "        self.adj_list[src].append(dest)\n",
    "        self.adj_list[dest].append(src)\n",
    "\n",
    "def dfs(graph, visited, v, traversal_order):\n",
    "    if not visited[v]:\n",
    "        visited[v] = 1\n",
    "        traversal_order.append(v)\n",
    "        for adj in graph.adj_list[v]:\n",
    "            dfs(graph, visited, adj, traversal_order)\n",
    "\n",
    "def parallel_dfs(graph, start):\n",
    "    num_nodes = len(graph.adj_list)\n",
    "    visited = Array('i', [0] * num_nodes)\n",
    "    traversal_order = []\n",
    "\n",
    "    # Start parallel DFS traversal from all vertices\n",
    "    processes = []\n",
    "    for v in range(num_nodes):\n",
    "        if v != start:\n",
    "            p = Process(target=dfs, args=(graph, visited, v, traversal_order))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "\n",
    "    # Start DFS traversal from the start vertex\n",
    "    dfs(graph, visited, start, traversal_order)\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Print DFS traversal\n",
    "    print(\"Parallel DFS from vertex\", start, \":\", traversal_order)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_nodes = 6\n",
    "    graph = Graph(num_nodes)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(5, 1)\n",
    "    graph.add_edge(5, 3)\n",
    "    graph.add_edge(1, 2)\n",
    "    graph.add_edge(1, 4)\n",
    "    graph.add_edge(3, 4)\n",
    "    graph.add_edge(2, 0)\n",
    "    graph.add_edge(4, 0)\n",
    "\n",
    "    start_vertex = int(input(\"Enter the start vertex: \"))  # Take input for the starting vertex\n",
    "\n",
    "    # Perform parallel DFS\n",
    "    parallel_dfs(graph, start_vertex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebd54da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the start vertex: 5\n",
      "BFS traversal from vertex 5 : [5, 1, 3, 2, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, num_nodes):\n",
    "        self.adj_list = [[] for _ in range(num_nodes)]\n",
    "\n",
    "    def add_edge(self, src, dest):\n",
    "        self.adj_list[src].append(dest)\n",
    "        self.adj_list[dest].append(src)\n",
    "\n",
    "def bfs(graph, start):\n",
    "    num_nodes = len(graph.adj_list)\n",
    "    visited = [False] * num_nodes\n",
    "    traversal_order = []\n",
    "\n",
    "    queue = Queue()\n",
    "    queue.put(start)\n",
    "    visited[start] = True\n",
    "\n",
    "    while not queue.empty():\n",
    "        v = queue.get()\n",
    "        traversal_order.append(v)\n",
    "        for adj in graph.adj_list[v]:\n",
    "            if not visited[adj]:\n",
    "                visited[adj] = True\n",
    "                queue.put(adj)\n",
    "\n",
    "    return traversal_order\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_nodes = 6\n",
    "    graph = Graph(num_nodes)\n",
    "\n",
    "    # Add edges\n",
    "    graph.add_edge(5, 1)\n",
    "    graph.add_edge(5, 3)\n",
    "    graph.add_edge(1, 2)\n",
    "    graph.add_edge(1, 4)\n",
    "    graph.add_edge(3, 4)\n",
    "    graph.add_edge(2, 0)\n",
    "    graph.add_edge(4, 0)\n",
    "\n",
    "    start_vertex = int(input(\"Enter the start vertex: \"))  # Take input for the starting vertex\n",
    "\n",
    "    # Perform BFS\n",
    "    traversal_order = bfs(graph, start_vertex)\n",
    "    print(\"BFS traversal from vertex\", start_vertex, \":\", traversal_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218d5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbad793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: [6, 4, 25, 1, 2, 11, 9]\n",
      "Sorted array bubble: [1, 2, 4, 6, 9, 11, 25]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def bubble_sort(arr):\n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j+1]:\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
    "\n",
    "def parallel_bubble_sort(arr):\n",
    "    # Split the array into chunks for parallel processing\n",
    "    num_chunks = multiprocessing.cpu_count()\n",
    "    chunk_size = max(1, len(arr) // num_chunks)  # Ensure chunk size is at least 1\n",
    "    chunks = [arr[i:i + chunk_size] for i in range(0, len(arr), chunk_size)]\n",
    "\n",
    "    # Create processes for each chunk\n",
    "    processes = []\n",
    "    for chunk in chunks:\n",
    "        process = multiprocessing.Process(target=bubble_sort, args=(chunk,))\n",
    "        process.start()\n",
    "        processes.append(process)\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    # Merge sorted chunks\n",
    "    sorted_arr = merge_chunks(chunks)\n",
    "    return sorted_arr\n",
    "\n",
    "def merge_chunks(chunks):\n",
    "    sorted_arr = []\n",
    "    while any(chunks):\n",
    "        min_val = float('inf')\n",
    "        min_index = -1\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if chunk and chunk[0] < min_val:\n",
    "                min_val = chunk[0]\n",
    "                min_index = i\n",
    "        sorted_arr.append(chunks[min_index].pop(0))\n",
    "    return sorted_arr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arr = [6, 4, 25, 1, 2, 11, 9]\n",
    "    print(\"Original array:\", arr)\n",
    "    sorted_arr = parallel_bubble_sort(arr)\n",
    "    print(\"Sorted array bubble:\", sorted_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1afb5460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: [640, 34, 2555, 12, 22, 11, 90]\n",
      "Sorted array merge: [11, 12, 22, 34, 90, 640, 2555]\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "def merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    mid = len(arr) // 2\n",
    "    left_half = arr[:mid]\n",
    "    right_half = arr[mid:]\n",
    "\n",
    "    # Recursively sort the left and right halves\n",
    "    left_half = merge_sort(left_half)\n",
    "    right_half = merge_sort(right_half)\n",
    "\n",
    "    # Merge the sorted halves\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "def merge(left, right):\n",
    "    merged = []\n",
    "    left_index = right_index = 0\n",
    "\n",
    "    # Merge the left and right subarrays\n",
    "    while left_index < len(left) and right_index < len(right):\n",
    "        if left[left_index] < right[right_index]:\n",
    "            merged.append(left[left_index])\n",
    "            left_index += 1\n",
    "        else:\n",
    "            merged.append(right[right_index])\n",
    "            right_index += 1\n",
    "\n",
    "    # Append remaining elements from left and right subarrays\n",
    "    merged.extend(left[left_index:])\n",
    "    merged.extend(right[right_index:])\n",
    "\n",
    "    return merged\n",
    "\n",
    "def parallel_merge_sort(arr):\n",
    "    # Split the array into chunks for parallel processing\n",
    "    num_chunks = multiprocessing.cpu_count()\n",
    "    chunk_size = max(1, len(arr) // num_chunks)  # Ensure chunk size is at least 1\n",
    "    chunks = [arr[i:i + chunk_size] for i in range(0, len(arr), chunk_size)]\n",
    "\n",
    "    # Use ThreadPoolExecutor to perform parallel merge sort\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        sorted_chunks = list(executor.map(merge_sort, chunks))  # Convert generator to list\n",
    "\n",
    "    # Merge the sorted chunks sequentially\n",
    "    sorted_arr = merge_chunks(sorted_chunks)\n",
    "    return sorted_arr\n",
    "\n",
    "def merge_chunks(chunks):\n",
    "    sorted_arr = chunks[0]\n",
    "    for chunk in chunks[1:]:\n",
    "        sorted_arr = merge(sorted_arr, chunk)\n",
    "    return sorted_arr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    arr = [640, 34, 2555, 12, 22, 11, 90]\n",
    "    print(\"Original array:\", arr)\n",
    "    sorted_arr = parallel_merge_sort(arr)\n",
    "    print(\"Sorted array merge:\", sorted_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e783d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
